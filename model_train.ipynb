{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"/Users/cameliamazouz/Documents/M2/machine_learning/multinli_1.0/multinli_1.0_train.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6835f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb9328",
   "metadata": {},
   "source": [
    "#### lemmatisation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba43220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de lemmatisation (NLTK)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    # Tentative de téléchargement silencieux des ressources utiles\n",
    "    for pkg in [\"punkt\", \"wordnet\", \"omw-1.4\", \"averaged_perceptron_tagger\", \"averaged_perceptron_tagger_eng\"]:\n",
    "        try:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def to_wordnet_pos(treebank_tag: str):\n",
    "    # Map POS tag PennTreebank vers WordNet POS\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN  # défaut raisonnable\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = pos_tag(tokens)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(tok, pos=to_wordnet_pos(tag)) for tok, tag in tags]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def lemmatize_columns(df, columns=(\"sentence1\", \"sentence2\")):\n",
    "    ensure_nltk_resources()\n",
    "    df_out = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df_out.columns:\n",
    "            df_out[f\"{col}_lemma\"] = df_out[col].astype(str).apply(lemmatize_text)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6747b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la lemmatisation à df et sauvegarder\n",
    "# Assurez-vous d'avoir exécuté les cellules qui définissent df\n",
    "df_lemma = lemmatize_columns(df, columns=(\"sentence1\", \"sentence2\"))\n",
    "print(\"Colonnes disponibles:\", df_lemma.columns.tolist())\n",
    "print(\"Aperçu lemmatisé:\")\n",
    "display(df_lemma[[\"sentence1\", \"sentence1_lemma\", \"sentence2\", \"sentence2_lemma\" ]].head())\n",
    "\n",
    "# Sauvegarde\n",
    "import os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "out_lemma_csv = \"artifacts/df_lemmatized.csv\"\n",
    "df_lemma.to_csv(out_lemma_csv, index=False)\n",
    "print(f\"Sauvegardé: {out_lemma_csv} (shape: {df_lemma.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"genre_id\"] = df[\"genre\"].astype(\"category\").cat.codes\n",
    "# df[\"label_id\"] = df[\"gold_label\"].astype(\"category\").cat.codes\n",
    "\n",
    "# # Vérification\n",
    "# print(df[[\"genre\", \"genre_id\", \"gold_label\", \"label_id\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efa592",
   "metadata": {},
   "source": [
    "#### Vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "df[\"label_id\"] = df[\"gold_label\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Nettoyage rapide (les NaNs font planter le vectorizer)\n",
    "df[\"sentence1\"] = df[\"sentence1\"].fillna(\"\")\n",
    "df[\"sentence2\"] = df[\"sentence2\"].fillna(\"\")\n",
    "\n",
    "# 2. Création du Vectorizer\n",
    "# On peut ajouter stop_words='english' pour virer les \"the\", \"is\", \"a\"...\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000) \n",
    "\n",
    "# 3. Apprentissage du vocabulaire sur TOUT le texte (S1 + S2)\n",
    "# C'est crucial pour que la colonne 42 corresponde au mot \"apple\" dans les deux vecteurs\n",
    "all_text = pd.concat([df[\"sentence1\"], df[\"sentence2\"]])\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "# 4. Transformation en vecteurs de fréquence\n",
    "# X1 = Fréquence des mots dans sentence1\n",
    "# X2 = Fréquence des mots dans sentence2 (en utilisant le même dictionnaire que S1)\n",
    "X1 = vectorizer.transform(df[\"sentence1\"])\n",
    "X2 = vectorizer.transform(df[\"sentence2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# On choisit l'index que tu voulais (1)\n",
    "idx = 1\n",
    "\n",
    "# 1. Récupérer le dictionnaire (la liste des mots dans l'ordre des colonnes)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 2. Récupérer les comptes pour la ligne choisie\n",
    "# .flatten() permet d'aplatir le tableau (convertir [[0,1]] en [0,1])\n",
    "compte_s1 = X1[idx].toarray().flatten()\n",
    "compte_s2 = X2[idx].toarray().flatten()\n",
    "\n",
    "# 3. Créer un tableau propre pour l'affichage\n",
    "df_visu = pd.DataFrame({\n",
    "    'Mot': vocabulaire,\n",
    "    'S1 (Freq)': compte_s1,\n",
    "    'S2 (Freq)': compte_s2\n",
    "})\n",
    "\n",
    "# 4. FILTRAGE : On n'affiche que les mots présents dans au moins l'une des phrases\n",
    "# (Sinon on va afficher 990 lignes de zéros)\n",
    "mask = (df_visu['S1 (Freq)'] > 0) | (df_visu['S2 (Freq)'] > 0)\n",
    "df_resultat = df_visu[mask].sort_values(by='S1 (Freq)', ascending=False)\n",
    "\n",
    "# --- AFFICHAGE ---\n",
    "print(f\"--- Analyse de l'index {idx} ---\\n\")\n",
    "print(f\"Phrase 1 : \\\"{df['sentence1'].iloc[idx]}\\\"\")\n",
    "print(f\"Phrase 2 : \\\"{df['sentence2'].iloc[idx]}\\\"\\n\")\n",
    "print(\"Mots comptés (intersection avec le vocabulaire connu) :\")\n",
    "print(df_resultat.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd204c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# On colle les matrices horizontalement : [Features S1] + [Features S2]\n",
    "X_combined = hstack([X1, X2])\n",
    "y = df[\"label_id\"]\n",
    "\n",
    "print(f\"Split des données (10% test sur {len(df)} lignes)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.10, random_state=42\n",
    ")\n",
    "\n",
    "# 6. ENTRAÎNEMENT (Régression Logistique)\n",
    "print(\"Entraînement du modèle...\")\n",
    "model = LogisticRegression(max_iter=1000) # max_iter augmenté pour la convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. ÉVALUATION\n",
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"\\n--- RÉSULTATS ---\")\n",
    "print(f\"Accuracy (Précision globale) : {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# Détail par classe\n",
    "target_names = [\"Entailment (0)\", \"Neutral (1)\", \"Contradiction (2)\"]\n",
    "print(\"\\nDétail par classe :\")\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
